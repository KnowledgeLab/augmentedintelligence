{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1 - Retreiving and Preparing Text for Machines\n",
    "\n",
    "This week, we begin by \"begging, borrowing and stealing\" text from several\n",
    "contexts of human communication (e.g., PDFs, HTML, Word) and preparing it for\n",
    "machines to \"read\" and analyze. This notebook outlines scraping text from the\n",
    "web, from images, PDF and Word documents. Then we detail \"spidering\" or walking\n",
    "through hyperlinks to build samples of online content, and using APIs,\n",
    "Application Programming Interfaces, provided by webservices to access their\n",
    "content. Along the way, we will use regular expressions, outlined in the\n",
    "reading, to remove unwanted formatting and ornamentation. Finally, we discuss\n",
    "various text encodings, filtering and data structures in which text can be\n",
    "placed for analysis.\n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called `beautifulsoup4`, an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "import PIL.Image #called `pillow`, an image processing library\n",
    "import pytesseract #an interface to Tesseract, you will need Tesseract\n",
    "import praw #For getting the reddit data\n",
    "import nltk #The Natural Language Toolkit, for dealing with text\n",
    "import numpy as np #Gives us arrays and some other nice math tools\n",
    "import matplotlib.pyplot as plt #Makes plots\n",
    "import seaborn #Makes the plots look nice\n",
    "\n",
    "#install as `scikit-learn`, provides a large collection of Machine learning tools\n",
    "#Each should be imported seperatly\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "#Stuff for pdfs\n",
    "#Install as `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib.parse #For joining urls\n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr API responses\n",
    "import os.path #For checking if files exist\n",
    "import os #For making directories\n",
    "import json #For loading the reddit account data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will also be working on the following files/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_intelligence_amplification = 'https://en.wikipedia.org/wiki/Intelligence_amplification'\n",
    "intelligence_amplification_save = 'data/intelligence_amplification_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'\n",
    "example_docx = 'https://github.com/KnowledgeLab/content_analysis/raw/data/example_doc.docx'\n",
    "example_docx_save = 'data/example.docx'\n",
    "ocr_file = 'data/tesseractTest.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Scraping\n",
    "\n",
    "Before we can start analyzing content we need to obtain it. Sometimes it will be\n",
    "provided to us from a pre-curated text archive, but sometimes we will need to\n",
    "download it. As a starting example we will attempt to download the wikipedia\n",
    "page on content analysis. The page is located at [https://en.wikipedia.org/wiki/Intelligence_amplification](https://en.wikipedia.org/wiki/Intelligence_amplification) so lets start\n",
    "with that.\n",
    "\n",
    "We can do this by making an HTTP GET request to that url, a GET request is\n",
    "simply a request to the server to provide the contents given by some url. The\n",
    "other request we will be using in this class is called a POST request and\n",
    "requests the server to take some content we provide. While the Python standard\n",
    "library does have the ability do make GET requests we will be using the\n",
    "[_requests_](http://docs.python-requests.org/en/master/) package as it is _'the\n",
    "only Non-GMO HTTP library for Python'_...also it provides a nicer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_intelligence_amplification = 'https://en.wikipedia.org/wiki/Intelligence_amplification'\n",
    "requests.get(wikipedia_intelligence_amplification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`'Response [200]'` means the server responded with what we asked for. If you get\n",
    "another number (e.g. 404) it likely means there was some kind of error, these\n",
    "codes are called HTTP response codes and a list of them can be found\n",
    "[here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). The response\n",
    "object contains all the data the server sent including the website's contents\n",
    "and the HTTP header. We are interested in the contents which we can access with\n",
    "the `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Intelligence amplification - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Intelligence_amplification\",\"wgTitle\":\"Intelligence amplification\",\"wgCurRevisionId\":731793777,\"wgRevisionId\":731793777,\"wgArticleId\":3948917,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"History of human–computer interaction\",\"Cybernetics\",\"Biocybernetics\",\"Transhumanism\",\"Texts related to the history of the Internet\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wg\n"
     ]
    }
   ],
   "source": [
    "wikiIntRequest = requests.get(wikipedia_intelligence_amplification)\n",
    "print(wikiIntRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is not what we were looking for, because it is the start of the HTML that\n",
    "makes up the website. This is HTML and is meant to be read by computers. Luckily\n",
    "we have a computer to parse it for us. To do the parsing we will use [_Beautiful\n",
    "Soup_](https://www.crummy.com/software/BeautifulSoup/) which is a better parser\n",
    "than the one in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Intelligence amplification - Wikipedia\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "(window.RLQ=window.RLQ||[]).pu\n"
     ]
    }
   ],
   "source": [
    "wikiIntSoup = bs4.BeautifulSoup(wikiIntRequest.text, 'html.parser')\n",
    "print(wikiIntSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is better but there's still random whitespace and we have more than just\n",
    "the text of the article. This is because what we requested is the whole webpage,\n",
    "not just the text for the article.\n",
    "\n",
    "We want to extract only the text we care about, and in order to do this we will\n",
    "need to inspect the html. One way to do this is simply to go to the website with\n",
    "a browser and use its inspection or view source tool. If javascript or other\n",
    "dynamic loading occurs on the page, however, it is likely that what Python\n",
    "receives is not what you will see, so we will need to inspect what Python\n",
    "receives. To do this we can save the html `requests` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#intelligence_amplification_save = 'data/intelligence_amplification_analysis.html'\n",
    "\n",
    "with open(intelligence_amplification_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiIntRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now lets open the file (`data/intelligence_amplification_analysis.html`) we just created with\n",
    "a web browser. It should look sort of like the original but without the images\n",
    "and formatting.\n",
    "\n",
    "As there is very little standardization on structuring webpages, figuring out\n",
    "how best to extract what you want is an art. Looking at this page it looks like\n",
    "all the main textual content is inside `<p>`(paragraph) tags within the `<body>`\n",
    "tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligence amplification (IA) (also referred to as cognitive augmentation and machine augmented intelligence) refers to the effective use of information technology in augmenting human intelligence. The idea was first proposed in the 1950s and 1960s by cybernetics and early computer pioneers.\n",
      "IA is sometimes contrasted with AI (artificial intelligence), that is, the project of building a human-like intelligence in the form of an autonomous technological system such as a computer or robot. AI has encountered many fundamental obstacles, practical as well as theoretical, which for IA seem moot, as it needs technology merely as an extra support for an autonomous intelligence that has already proven to function. Moreover, IA has a long history of success, since all forms of information technology, from the abacus to writing to the Internet, have been developed basically to extend the information processing capabilities of the human mind (see extended mind and distributed cognition).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intPTags = wikiIntSoup.body.findAll('p')\n",
    "for pTag in intPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have all the text from the page, split up by paragraph. If we wanted to\n",
    "get the section headers or references as well it would require a bit more work,\n",
    "but is doable.\n",
    "\n",
    "There is one more thing we might want to do before sending this text to be\n",
    "processed, remove the references indicators (`[2]`, `[3]` , etc). To do this we\n",
    "can use a short regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       paragraph-text\n",
      "0   Intelligence amplification (IA) (also referred...\n",
      "1   IA is sometimes contrasted with AI (artificial...\n",
      "2                                                    \n",
      "3                                                    \n",
      "4   The term intelligence amplification (IA) has e...\n",
      "5   ..\"problem solving\" is largely, perhaps entire...\n",
      "6   It is also clear that many of the tests used f...\n",
      "7   If this is so, and as we know that power of se...\n",
      "8   \"Man-Computer Symbiosis\" is a key speculative ...\n",
      "9   Man-computer symbiosis is a subclass of man-ma...\n",
      "10  In Licklider's vision, many of the pure artifi...\n",
      "11  Licklider's research was similar in spirit to ...\n",
      "12  Engelbart reasoned that the state of our curre...\n",
      "13  \"Increasing the capability of a man to approac...\n",
      "14  Increased capability in this respect is taken ...\n",
      "15  We do not speak of isolated clever tricks that...\n",
      "16  Engelbart subsequently implemented these conce...\n"
     ]
    }
   ],
   "source": [
    "intParagraphs = []\n",
    "for pTag in intPTags:\n",
    "    #strings starting with r are raw so their \\'s are not modifier characters\n",
    "    #If we didn't start with r the string would be: '\\\\[\\\\d+\\\\]'\n",
    "    intParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#convert to a DataFrame\n",
    "intParagraphsDF = pandas.DataFrame({'paragraph-text' : intParagraphs})\n",
    "print(intParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have a `DataFrame` containing all relevant text from the page ready to be\n",
    "processed\n",
    "\n",
    "If you are not familiar with regex, it is a way of specifying searches in text.\n",
    "A regex engine takes in the search pattern, in the above case `'\\[\\d+\\]'` and\n",
    "some string, the paragraph texts. Then it reads the input string one character\n",
    "at a time checking if it matches the search. Here the regex `'\\d'` matches\n",
    "number characters (while `'\\['` and `'\\]'` capture the braces on either side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(36, 37), match='2'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In Python the regex package (`re`) usually returns `Match` objects (you can have\n",
    "multiple pattern hits in a a single `Match`), to get the string that matched our\n",
    "pattern we can use the `.group()` method, and as we want the first one we will\n",
    "ask for the 0'th group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That gives us the first number, if we wanted the whole block of numbers we can\n",
    "add a wildcard `'+'` which requests 1 or more instances of the preceding\n",
    "character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134567890\n"
     ]
    }
   ],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have the whole block of numbers, there are a huge number of special\n",
    "characters in regex, for the full description of Python's implementation look at\n",
    "the [re docs](https://docs.python.org/3/library/re.html) there is also a short\n",
    "[tutorial](https://docs.python.org/3/howto/regex.html#regex-howto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spidering\n",
    "\n",
    "What if we want to to get a bunch of different pages from wikipedia. We would\n",
    "need to get the url for each of the pages we want. Typically, we want pages that\n",
    "are linked to by other pages and so we will need to parse pages and identify the\n",
    "links. Right now we will be retrieving all links in the body of the content\n",
    "analysis page.\n",
    "\n",
    "To do this we will need to find all the `<a>` (anchor) tags with `href`s\n",
    "(hyperlink references) inside of `<p>` tags. `href` can have many\n",
    "[different](http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-\n",
    "it-used) [forms](https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML) so\n",
    "dealing with them can be tricky, but generally, you will want to extract\n",
    "absolute or relative links. An absolute link is one you can follow without\n",
    "modification, while a relative link requires a base url that you will then\n",
    "append. Wikipedia uses relative urls for its internal links: below is an example\n",
    "for dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://en.wikipedia.org/wiki/Information_technology', 0, 'information technology'), ('https://en.wikipedia.org/wiki/Intelligence#Human_intelligence', 0, 'human intelligence'), ('https://en.wikipedia.org/wiki/Cybernetics', 0, 'cybernetics'), ('https://en.wikipedia.org/wiki/Artificial_intelligence', 1, 'artificial intelligence'), ('https://en.wikipedia.org/wiki/Information_processing', 1, 'information processing'), ('https://en.wikipedia.org/wiki/Distributed_cognition', 1, 'distributed cognition'), ('https://en.wikipedia.org/wiki/Hao_Wang_(academic)', 4, 'Hao Wang'), ('https://en.wikipedia.org/wiki/Automated_theorem_proving', 4, 'automatic theorem provers'), ('https://en.wikipedia.org/wiki/Problem_solving', 5, 'problem solving'), ('https://en.wikipedia.org/wiki/Black_box', 6, 'Black Box')]\n"
     ]
    }
   ],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#We also want to know where the links come from so we also will get:\n",
    "#the paragraph number\n",
    "#the word the link is in\n",
    "for paragraphNum, pTag in enumerate(intPTags):\n",
    "    #we only want hrefs that link to wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #We need to extract the url from the <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #wikipedia_base_url is the base we can use the urllib joining function to merge them\n",
    "        #Giving a nice structured tupe like this means we can use tuple expansion later\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will be adding these new texts to our DataFrame `contentParagraphsDF` so we\n",
    "will need to add 2 more columns to keep track of paragraph numbers and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>paragraph-number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intelligence amplification (IA) (also referred...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IA is sometimes contrasted with AI (artificial...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The term intelligence amplification (IA) has e...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>..\"problem solving\" is largely, perhaps entire...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It is also clear that many of the tests used f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>If this is so, and as we know that power of se...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Man-Computer Symbiosis\" is a key speculative ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Man-computer symbiosis is a subclass of man-ma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>In Licklider's vision, many of the pure artifi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Licklider's research was similar in spirit to ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Engelbart reasoned that the state of our curre...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Increasing the capability of a man to approac...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Increased capability in this respect is taken ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>We do not speak of isolated clever tricks that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Engelbart subsequently implemented these conce...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paragraph-text  \\\n",
       "0   Intelligence amplification (IA) (also referred...   \n",
       "1   IA is sometimes contrasted with AI (artificial...   \n",
       "2                                                       \n",
       "3                                                       \n",
       "4   The term intelligence amplification (IA) has e...   \n",
       "5   ..\"problem solving\" is largely, perhaps entire...   \n",
       "6   It is also clear that many of the tests used f...   \n",
       "7   If this is so, and as we know that power of se...   \n",
       "8   \"Man-Computer Symbiosis\" is a key speculative ...   \n",
       "9   Man-computer symbiosis is a subclass of man-ma...   \n",
       "10  In Licklider's vision, many of the pure artifi...   \n",
       "11  Licklider's research was similar in spirit to ...   \n",
       "12  Engelbart reasoned that the state of our curre...   \n",
       "13  \"Increasing the capability of a man to approac...   \n",
       "14  Increased capability in this respect is taken ...   \n",
       "15  We do not speak of isolated clever tricks that...   \n",
       "16  Engelbart subsequently implemented these conce...   \n",
       "\n",
       "                                               source  paragraph-number  \n",
       "0   https://en.wikipedia.org/wiki/Intelligence_amp...                 0  \n",
       "1   https://en.wikipedia.org/wiki/Intelligence_amp...                 1  \n",
       "2   https://en.wikipedia.org/wiki/Intelligence_amp...                 2  \n",
       "3   https://en.wikipedia.org/wiki/Intelligence_amp...                 3  \n",
       "4   https://en.wikipedia.org/wiki/Intelligence_amp...                 4  \n",
       "5   https://en.wikipedia.org/wiki/Intelligence_amp...                 5  \n",
       "6   https://en.wikipedia.org/wiki/Intelligence_amp...                 6  \n",
       "7   https://en.wikipedia.org/wiki/Intelligence_amp...                 7  \n",
       "8   https://en.wikipedia.org/wiki/Intelligence_amp...                 8  \n",
       "9   https://en.wikipedia.org/wiki/Intelligence_amp...                 9  \n",
       "10  https://en.wikipedia.org/wiki/Intelligence_amp...                10  \n",
       "11  https://en.wikipedia.org/wiki/Intelligence_amp...                11  \n",
       "12  https://en.wikipedia.org/wiki/Intelligence_amp...                12  \n",
       "13  https://en.wikipedia.org/wiki/Intelligence_amp...                13  \n",
       "14  https://en.wikipedia.org/wiki/Intelligence_amp...                14  \n",
       "15  https://en.wikipedia.org/wiki/Intelligence_amp...                15  \n",
       "16  https://en.wikipedia.org/wiki/Intelligence_amp...                16  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intParagraphsDF['source'] = [wikipedia_intelligence_amplification] * len(intParagraphsDF['paragraph-text'])\n",
    "intParagraphsDF['paragraph-number'] = range(len(intParagraphsDF['paragraph-text']))\n",
    "\n",
    "intParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we can add two more columns to our `Dataframe` and define a function to parse\n",
    "each linked page and add its text to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intParagraphsDF['source-paragraph-number'] = [None] * len(intParagraphsDF['paragraph-text'])\n",
    "intParagraphsDF['source-paragraph-text'] = [None] * len(intParagraphsDF['paragraph-text'])\n",
    "\n",
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Make a dict to store data before adding it to the DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Now we get the page\n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #enumerating gives use the paragraph number\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #same regex as before\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And run it on our list of link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Intelligence amplification (IA) (also referred...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IA is sometimes contrasted with AI (artificial...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The term intelligence amplification (IA) has e...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>..\"problem solving\" is largely, perhaps entire...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>It is also clear that many of the tests used f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>If this is so, and as we know that power of se...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>\"Man-Computer Symbiosis\" is a key speculative ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Man-computer symbiosis is a subclass of man-ma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>In Licklider's vision, many of the pure artifi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Licklider's research was similar in spirit to ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Engelbart reasoned that the state of our curre...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>\"Increasing the capability of a man to approac...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Increased capability in this respect is taken ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>We do not speak of isolated clever tricks that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Engelbart subsequently implemented these conce...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Intelligence_amp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>Information technology (IT) is the application...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>The term is commonly used as a synonym for com...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Humans have been storing, retrieving, manipula...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>Based on the storage and processing technologi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Devices have been used to aid computation for ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>Electronic computers, using either relays or v...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>The development of transistors in the late 194...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9</td>\n",
       "      <td>Early electronic computers such as Colossus ma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>IBM introduced the first hard disk drive in 19...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11</td>\n",
       "      <td>Database management systems emerged in the 196...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>All database management systems consist of a n...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_tech...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>14</td>\n",
       "      <td>Early applications of negative feedback in ele...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>15</td>\n",
       "      <td>W. Edwards Deming, the Total Quality Managemen...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>16</td>\n",
       "      <td>Numerous papers spearheaded the coalescing of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>17</td>\n",
       "      <td>In 1936, Ștefan Odobleja publishes \"Phonoscopy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18</td>\n",
       "      <td>Cybernetics as a discipline was firmly establi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>19</td>\n",
       "      <td>In the early 1940s John von Neumann, although ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>20</td>\n",
       "      <td>In 1950, Wiener popularized the social implica...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>21</td>\n",
       "      <td>In the Soviet Union cybernetics was considered...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>22</td>\n",
       "      <td>While not the only instance of a research orga...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>23</td>\n",
       "      <td>Artificial intelligence (AI) was founded as a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>24</td>\n",
       "      <td>Prominent cyberneticians during this period in...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>25</td>\n",
       "      <td>In the 1970s, new cyberneticians emerged in mu...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>26</td>\n",
       "      <td>In political science, Project Cybersyn attempt...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>27</td>\n",
       "      <td>One characteristic of the emerging new cyberne...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>28</td>\n",
       "      <td>Recent endeavors into the true focus of cybern...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>29</td>\n",
       "      <td>The design of self-regulating control systems ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>30</td>\n",
       "      <td>More recent proposals for socialism involve \"N...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>31</td>\n",
       "      <td>Cybernetics is sometimes used as a generic ter...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>32</td>\n",
       "      <td>Cybernetics studies systems of control as a co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>33</td>\n",
       "      <td>Cybernetics in biology is the study of cyberne...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>34</td>\n",
       "      <td>Computer science directly applies the concepts...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>35</td>\n",
       "      <td>Cybernetics in engineering is used to analyze ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>36</td>\n",
       "      <td>Mathematical Cybernetics focuses on the factor...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>37</td>\n",
       "      <td>By examining group behavior through the lens o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>38</td>\n",
       "      <td>A model of cybernetics in Education was introd...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>39</td>\n",
       "      <td>Nicolas Schöffer's CYSP I (1956) was perhaps t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>40</td>\n",
       "      <td>Geocybernetics aims to study and control the c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>41</td>\n",
       "      <td>A model of cybernetics in Sport was introduced...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>42</td>\n",
       "      <td>Complexity science attempts to understand the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>43</td>\n",
       "      <td>Biomechatronics relates to linking mechatronic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cybernetics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cybernetics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paragraph-number                                     paragraph-text  \\\n",
       "0                   0  Intelligence amplification (IA) (also referred...   \n",
       "1                   1  IA is sometimes contrasted with AI (artificial...   \n",
       "2                   2                                                      \n",
       "3                   3                                                      \n",
       "4                   4  The term intelligence amplification (IA) has e...   \n",
       "5                   5  ..\"problem solving\" is largely, perhaps entire...   \n",
       "6                   6  It is also clear that many of the tests used f...   \n",
       "7                   7  If this is so, and as we know that power of se...   \n",
       "8                   8  \"Man-Computer Symbiosis\" is a key speculative ...   \n",
       "9                   9  Man-computer symbiosis is a subclass of man-ma...   \n",
       "10                 10  In Licklider's vision, many of the pure artifi...   \n",
       "11                 11  Licklider's research was similar in spirit to ...   \n",
       "12                 12  Engelbart reasoned that the state of our curre...   \n",
       "13                 13  \"Increasing the capability of a man to approac...   \n",
       "14                 14  Increased capability in this respect is taken ...   \n",
       "15                 15  We do not speak of isolated clever tricks that...   \n",
       "16                 16  Engelbart subsequently implemented these conce...   \n",
       "17                  0  Information technology (IT) is the application...   \n",
       "18                  1  The term is commonly used as a synonym for com...   \n",
       "19                  2  Humans have been storing, retrieving, manipula...   \n",
       "20                  3  Based on the storage and processing technologi...   \n",
       "21                  4                                                      \n",
       "22                  5                                                      \n",
       "23                  6  Devices have been used to aid computation for ...   \n",
       "24                  7  Electronic computers, using either relays or v...   \n",
       "25                  8  The development of transistors in the late 194...   \n",
       "26                  9  Early electronic computers such as Colossus ma...   \n",
       "27                 10  IBM introduced the first hard disk drive in 19...   \n",
       "28                 11  Database management systems emerged in the 196...   \n",
       "29                 12  All database management systems consist of a n...   \n",
       "..                ...                                                ...   \n",
       "92                 14  Early applications of negative feedback in ele...   \n",
       "93                 15  W. Edwards Deming, the Total Quality Managemen...   \n",
       "94                 16  Numerous papers spearheaded the coalescing of ...   \n",
       "95                 17  In 1936, Ștefan Odobleja publishes \"Phonoscopy...   \n",
       "96                 18  Cybernetics as a discipline was firmly establi...   \n",
       "97                 19  In the early 1940s John von Neumann, although ...   \n",
       "98                 20  In 1950, Wiener popularized the social implica...   \n",
       "99                 21  In the Soviet Union cybernetics was considered...   \n",
       "100                22  While not the only instance of a research orga...   \n",
       "101                23  Artificial intelligence (AI) was founded as a ...   \n",
       "102                24  Prominent cyberneticians during this period in...   \n",
       "103                25  In the 1970s, new cyberneticians emerged in mu...   \n",
       "104                26  In political science, Project Cybersyn attempt...   \n",
       "105                27  One characteristic of the emerging new cyberne...   \n",
       "106                28  Recent endeavors into the true focus of cybern...   \n",
       "107                29  The design of self-regulating control systems ...   \n",
       "108                30  More recent proposals for socialism involve \"N...   \n",
       "109                31  Cybernetics is sometimes used as a generic ter...   \n",
       "110                32  Cybernetics studies systems of control as a co...   \n",
       "111                33  Cybernetics in biology is the study of cyberne...   \n",
       "112                34  Computer science directly applies the concepts...   \n",
       "113                35  Cybernetics in engineering is used to analyze ...   \n",
       "114                36  Mathematical Cybernetics focuses on the factor...   \n",
       "115                37  By examining group behavior through the lens o...   \n",
       "116                38  A model of cybernetics in Education was introd...   \n",
       "117                39  Nicolas Schöffer's CYSP I (1956) was perhaps t...   \n",
       "118                40  Geocybernetics aims to study and control the c...   \n",
       "119                41  A model of cybernetics in Sport was introduced...   \n",
       "120                42  Complexity science attempts to understand the ...   \n",
       "121                43  Biomechatronics relates to linking mechatronic...   \n",
       "\n",
       "                                                source  \\\n",
       "0    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "1    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "2    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "3    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "4    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "5    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "6    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "7    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "8    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "9    https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "10   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "11   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "12   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "13   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "14   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "15   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "16   https://en.wikipedia.org/wiki/Intelligence_amp...   \n",
       "17   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "18   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "19   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "20   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "21   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "22   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "23   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "24   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "25   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "26   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "27   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "28   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "29   https://en.wikipedia.org/wiki/Information_tech...   \n",
       "..                                                 ...   \n",
       "92           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "93           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "94           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "95           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "96           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "97           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "98           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "99           https://en.wikipedia.org/wiki/Cybernetics   \n",
       "100          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "101          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "102          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "103          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "104          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "105          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "106          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "107          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "108          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "109          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "110          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "111          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "112          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "113          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "114          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "115          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "116          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "117          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "118          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "119          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "120          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "121          https://en.wikipedia.org/wiki/Cybernetics   \n",
       "\n",
       "     source-paragraph-number   source-paragraph-text  \n",
       "0                        NaN                    None  \n",
       "1                        NaN                    None  \n",
       "2                        NaN                    None  \n",
       "3                        NaN                    None  \n",
       "4                        NaN                    None  \n",
       "5                        NaN                    None  \n",
       "6                        NaN                    None  \n",
       "7                        NaN                    None  \n",
       "8                        NaN                    None  \n",
       "9                        NaN                    None  \n",
       "10                       NaN                    None  \n",
       "11                       NaN                    None  \n",
       "12                       NaN                    None  \n",
       "13                       NaN                    None  \n",
       "14                       NaN                    None  \n",
       "15                       NaN                    None  \n",
       "16                       NaN                    None  \n",
       "17                       0.0  information technology  \n",
       "18                       0.0  information technology  \n",
       "19                       0.0  information technology  \n",
       "20                       0.0  information technology  \n",
       "21                       0.0  information technology  \n",
       "22                       0.0  information technology  \n",
       "23                       0.0  information technology  \n",
       "24                       0.0  information technology  \n",
       "25                       0.0  information technology  \n",
       "26                       0.0  information technology  \n",
       "27                       0.0  information technology  \n",
       "28                       0.0  information technology  \n",
       "29                       0.0  information technology  \n",
       "..                       ...                     ...  \n",
       "92                       0.0             cybernetics  \n",
       "93                       0.0             cybernetics  \n",
       "94                       0.0             cybernetics  \n",
       "95                       0.0             cybernetics  \n",
       "96                       0.0             cybernetics  \n",
       "97                       0.0             cybernetics  \n",
       "98                       0.0             cybernetics  \n",
       "99                       0.0             cybernetics  \n",
       "100                      0.0             cybernetics  \n",
       "101                      0.0             cybernetics  \n",
       "102                      0.0             cybernetics  \n",
       "103                      0.0             cybernetics  \n",
       "104                      0.0             cybernetics  \n",
       "105                      0.0             cybernetics  \n",
       "106                      0.0             cybernetics  \n",
       "107                      0.0             cybernetics  \n",
       "108                      0.0             cybernetics  \n",
       "109                      0.0             cybernetics  \n",
       "110                      0.0             cybernetics  \n",
       "111                      0.0             cybernetics  \n",
       "112                      0.0             cybernetics  \n",
       "113                      0.0             cybernetics  \n",
       "114                      0.0             cybernetics  \n",
       "115                      0.0             cybernetics  \n",
       "116                      0.0             cybernetics  \n",
       "117                      0.0             cybernetics  \n",
       "118                      0.0             cybernetics  \n",
       "119                      0.0             cybernetics  \n",
       "120                      0.0             cybernetics  \n",
       "121                      0.0             cybernetics  \n",
       "\n",
       "[122 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #ignore_index means the indices will not be reset after each append\n",
    "    intParagraphsDF = intParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "intParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## API (Tumblr)\n",
    "\n",
    "Generally website owners do not like you scraping their sites. If done badly,\n",
    "scarping can act like a DOS attack so you should be careful how often you make\n",
    "calls to a site. Some sites want automated tools to access their data, so they\n",
    "create [application programming interface\n",
    "(APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). An API\n",
    "specifies a procedure for an application (or script) to access their data. Often\n",
    "this is though a [representational state transfer\n",
    "(REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web\n",
    "service, which just means if you make correctly formatted HTTP requests they\n",
    "will return nicely formatted data.\n",
    "\n",
    "A nice example for us to study is [Tumblr](https://www.tumblr.com), they have a\n",
    "[simple RESTful API](https://www.tumblr.com/docs/en/api/v1) that allows you to\n",
    "read posts without any complicated html parsing.\n",
    "\n",
    "We can get the first 20 posts from a blog by making an http GET request to\n",
    "`'http://{blog}.tumblr.com/api/read/json'`, were `{blog}` is the name of the\n",
    "target blog. Lets try and get the posts from [http://lolcats-lol-\n",
    "cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Note the blog says at the\n",
    "top 'One hour one pic lolcats', but the canonical name that Tumblr uses is in\n",
    "the URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tumblr_api_read = {\"tumblelog\":{\"title\":\"One hour one pic lolcats\",\"description\":\"\",\"name\":\"lolcats-lol-cat\",\"timezone\":\"Europe\\/Paris\",\"cname\":false,\"feeds\":[]},\"posts-start\":0,\"posts-total\":2719,\"posts-type\":false,\"posts\":[{\"id\":\"158722814874\",\"url\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/158722814874\",\"url-with-slug\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/158722814874\\/my-plan-to-infiltrate-has-succeeded\",\"type\":\"photo\",\"date-gmt\":\"2017-03-23 01:00:37 GMT\",\"date\":\"Thu, 23 Mar 2017 02:00:37\",\"bookmarklet\":0,\"mobile\":0,\"feed-item\":\"\",\"from-feed-id\":0,\"unix-timestamp\":1490230837,\"format\":\"html\",\"reblog-key\":\"efSGWGc7\",\"slug\":\"my-plan-to-infiltrate-has-succeeded\",\"is-submission\":false,\"like-button\":\"<div class=\\\"like_button\\\" data-post-id=\\\"158722814874\\\" data-blog-name=\\\"lolcats-lol-cat\\\" id=\\\"like_button_158722814874\\\"><iframe id=\\\"like_iframe_158722814874\\\" src=\\\"http:\\/\\/assets.tumblr.com\\/assets\\/html\\/like_iframe.html?_v=e350d333d34f56656df9f206c69335f3#name=lolcats-l\n"
     ]
    }
   ],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This might not look very good on first inspection, but it has far fewer angle\n",
    "braces than html, which makes it easier to parse. What we have is\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) a 'human readable' text based data\n",
    "transmission format based on javascript. Luckily, we can readily convert it to a\n",
    "python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['posts-total', 'posts', 'posts-start', 'posts-type', 'tumblelog'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#We need to load only the stuff between the curly braces\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we read the [API specification](https://www.tumblr.com/docs/en/api/v1), we\n",
    "will see there are a lot of things we can get if we add things to our GET\n",
    "request. First we can retrieve posts by their id number. Let's first get post\n",
    "`146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Such beauty; such vigor (If you can't see it you have to refresh the page). Now we could retrieve the text from all posts as well\n",
    "as related metadata, like the post date, caption or tags. We could also get\n",
    "links to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>photo-type</th>\n",
       "      <th>photo-url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu, 23 Mar 2017 02:00:37</td>\n",
       "      <td>158722814874</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/b74fc499f2c4b2775db...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue, 14 Mar 2017 10:00:23</td>\n",
       "      <td>158387804235</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/59f9b61d12fd5bdada4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun, 12 Mar 2017 22:00:32</td>\n",
       "      <td>158323420528</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/0d5dffb8caa823b1c8e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun, 12 Mar 2017 20:00:18</td>\n",
       "      <td>158318999781</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/86292e8c9b2fffbfdc1...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sun, 05 Mar 2017 18:00:39</td>\n",
       "      <td>158029314400</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/2761e107071e99101d3...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sun, 05 Mar 2017 16:00:22</td>\n",
       "      <td>158025254480</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/64d8c99f8106d50b0df...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sun, 05 Mar 2017 14:00:34</td>\n",
       "      <td>158022148996</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/db6cc862ec14d322866...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sun, 05 Mar 2017 12:00:24</td>\n",
       "      <td>158019789278</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/0ba4cfd33cb1e45cd04...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sun, 05 Mar 2017 10:00:13</td>\n",
       "      <td>158017230464</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/b411c3eab962762260d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sun, 05 Mar 2017 08:00:10</td>\n",
       "      <td>158014343099</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/dde5860cadf024ada4c...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sun, 05 Mar 2017 06:00:07</td>\n",
       "      <td>158010827897</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/9065f1a1a9b843738ee...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sun, 05 Mar 2017 04:00:28</td>\n",
       "      <td>158007006281</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/47b7f375b4149ab2bbd...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sat, 18 Feb 2017 18:00:49</td>\n",
       "      <td>157401637803</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/4b46a4fe1028eb1ea93...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sat, 18 Feb 2017 16:00:28</td>\n",
       "      <td>157397831831</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/9153d49c5566dada67f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sat, 18 Feb 2017 14:00:30</td>\n",
       "      <td>157394705757</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/89a1910a3816fad204b...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sat, 18 Feb 2017 12:00:13</td>\n",
       "      <td>157392270014</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/cb202df1921795ccd3f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tue, 07 Feb 2017 10:00:30</td>\n",
       "      <td>156927103609</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6b0d9fcc89c82c966e5...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tue, 07 Feb 2017 08:00:28</td>\n",
       "      <td>156924496394</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/222188615a566104b71...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tue, 07 Feb 2017 06:00:16</td>\n",
       "      <td>156920709636</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6d571d7686f237400b8...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tue, 07 Feb 2017 04:00:19</td>\n",
       "      <td>156916275161</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/d5cd8e8e6c242630ce7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Tue, 07 Feb 2017 02:00:26</td>\n",
       "      <td>156911901612</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/a11791191e0812ce384...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Tue, 31 Jan 2017 04:00:14</td>\n",
       "      <td>156608441704</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6b692cf29778f2e7f0d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tue, 31 Jan 2017 02:00:39</td>\n",
       "      <td>156603922715</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ca686d3fd9620610d7f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mon, 30 Jan 2017 22:00:43</td>\n",
       "      <td>156594682447</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/af74e12415dc53f29b8...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Mon, 23 Jan 2017 20:00:40</td>\n",
       "      <td>156273570535</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/1367a126e535ea8ab9d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fri, 20 Jan 2017 02:00:22</td>\n",
       "      <td>156102187402</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/5176bff2db2a2347d9c...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Thu, 19 Jan 2017 22:00:43</td>\n",
       "      <td>156093434729</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/bab620eef666b11675a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Mon, 16 Jan 2017 08:00:12</td>\n",
       "      <td>155935104806</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/b5d0166250e4a0d3488...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mon, 16 Jan 2017 06:00:11</td>\n",
       "      <td>155930706526</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/de430bb20a063438fb9...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Mon, 16 Jan 2017 04:00:30</td>\n",
       "      <td>155926025004</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/d0396cd266b8cc19c37...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mon, 16 Jan 2017 02:00:35</td>\n",
       "      <td>155921453887</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ae42ce48ff8acf0dc93...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tue, 10 Jan 2017 04:00:19</td>\n",
       "      <td>155653404265</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/5b1bb5a22fc533eb19a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tue, 10 Jan 2017 02:00:38</td>\n",
       "      <td>155648746267</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/483f023c438e5c8dd40...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sun, 08 Jan 2017 20:00:22</td>\n",
       "      <td>155585596433</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/559b0e1ee203204a3a5...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Wed, 04 Jan 2017 12:00:23</td>\n",
       "      <td>155385929782</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ee2920651e9b3d4e364...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tue, 03 Jan 2017 02:00:35</td>\n",
       "      <td>155320542732</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/7567ddcbee4af447779...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Mon, 02 Jan 2017 22:00:53</td>\n",
       "      <td>155310651518</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/a2a9e83e75b3d8915ac...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sat, 31 Dec 2016 08:00:34</td>\n",
       "      <td>155198080351</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/0c59f1654b492097c58...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sat, 31 Dec 2016 06:00:33</td>\n",
       "      <td>155194054871</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/e1c705ca985b11c1e1a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Tue, 20 Dec 2016 22:00:44</td>\n",
       "      <td>154733970182</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/bfc00e121ede86ec9d4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Tue, 20 Dec 2016 20:00:31</td>\n",
       "      <td>154729675124</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/5f7e855d512ab2b2dc4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Mon, 12 Dec 2016 22:00:33</td>\n",
       "      <td>154390040542</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6fb9ab5f5b041be83d7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mon, 12 Dec 2016 20:00:41</td>\n",
       "      <td>154385661374</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ba07f00e8ce8e4da765...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Fri, 09 Dec 2016 04:00:15</td>\n",
       "      <td>154230465752</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/7530c2210673a58e392...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Fri, 09 Dec 2016 02:00:30</td>\n",
       "      <td>154226290124</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b2a5e62ef55a6d176c3...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Tue, 06 Dec 2016 14:00:22</td>\n",
       "      <td>154117927413</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/cc44f2ff65366ec6b81...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Tue, 06 Dec 2016 12:00:43</td>\n",
       "      <td>154115515601</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/fef80b257374162c757...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Tue, 06 Dec 2016 10:00:40</td>\n",
       "      <td>154113363187</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/e2fa3b7422a9873dff3...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Tue, 06 Dec 2016 08:00:15</td>\n",
       "      <td>154110609304</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/54068be23b407110229...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Tue, 06 Dec 2016 06:00:14</td>\n",
       "      <td>154106783936</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/3bb935758ee6eb31cdf...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date            id photo-type  \\\n",
       "0   Thu, 23 Mar 2017 02:00:37  158722814874        png   \n",
       "1   Tue, 14 Mar 2017 10:00:23  158387804235        png   \n",
       "2   Sun, 12 Mar 2017 22:00:32  158323420528        jpg   \n",
       "3   Sun, 12 Mar 2017 20:00:18  158318999781        jpg   \n",
       "4   Sun, 05 Mar 2017 18:00:39  158029314400        jpg   \n",
       "5   Sun, 05 Mar 2017 16:00:22  158025254480        jpg   \n",
       "6   Sun, 05 Mar 2017 14:00:34  158022148996        jpg   \n",
       "7   Sun, 05 Mar 2017 12:00:24  158019789278        jpg   \n",
       "8   Sun, 05 Mar 2017 10:00:13  158017230464        jpg   \n",
       "9   Sun, 05 Mar 2017 08:00:10  158014343099        jpg   \n",
       "10  Sun, 05 Mar 2017 06:00:07  158010827897        jpg   \n",
       "11  Sun, 05 Mar 2017 04:00:28  158007006281        jpg   \n",
       "12  Sat, 18 Feb 2017 18:00:49  157401637803        jpg   \n",
       "13  Sat, 18 Feb 2017 16:00:28  157397831831        jpg   \n",
       "14  Sat, 18 Feb 2017 14:00:30  157394705757        jpg   \n",
       "15  Sat, 18 Feb 2017 12:00:13  157392270014        jpg   \n",
       "16  Tue, 07 Feb 2017 10:00:30  156927103609        jpg   \n",
       "17  Tue, 07 Feb 2017 08:00:28  156924496394        jpg   \n",
       "18  Tue, 07 Feb 2017 06:00:16  156920709636        jpg   \n",
       "19  Tue, 07 Feb 2017 04:00:19  156916275161        png   \n",
       "20  Tue, 07 Feb 2017 02:00:26  156911901612        jpg   \n",
       "21  Tue, 31 Jan 2017 04:00:14  156608441704        jpg   \n",
       "22  Tue, 31 Jan 2017 02:00:39  156603922715        jpg   \n",
       "23  Mon, 30 Jan 2017 22:00:43  156594682447        jpg   \n",
       "24  Mon, 23 Jan 2017 20:00:40  156273570535        jpg   \n",
       "25  Fri, 20 Jan 2017 02:00:22  156102187402        jpg   \n",
       "26  Thu, 19 Jan 2017 22:00:43  156093434729        jpg   \n",
       "27  Mon, 16 Jan 2017 08:00:12  155935104806        jpg   \n",
       "28  Mon, 16 Jan 2017 06:00:11  155930706526        jpg   \n",
       "29  Mon, 16 Jan 2017 04:00:30  155926025004        jpg   \n",
       "30  Mon, 16 Jan 2017 02:00:35  155921453887        jpg   \n",
       "31  Tue, 10 Jan 2017 04:00:19  155653404265        jpg   \n",
       "32  Tue, 10 Jan 2017 02:00:38  155648746267        jpg   \n",
       "33  Sun, 08 Jan 2017 20:00:22  155585596433        jpg   \n",
       "34  Wed, 04 Jan 2017 12:00:23  155385929782        jpg   \n",
       "35  Tue, 03 Jan 2017 02:00:35  155320542732        jpg   \n",
       "36  Mon, 02 Jan 2017 22:00:53  155310651518        jpg   \n",
       "37  Sat, 31 Dec 2016 08:00:34  155198080351        jpg   \n",
       "38  Sat, 31 Dec 2016 06:00:33  155194054871        jpg   \n",
       "39  Tue, 20 Dec 2016 22:00:44  154733970182        jpg   \n",
       "40  Tue, 20 Dec 2016 20:00:31  154729675124        png   \n",
       "41  Mon, 12 Dec 2016 22:00:33  154390040542        jpg   \n",
       "42  Mon, 12 Dec 2016 20:00:41  154385661374        jpg   \n",
       "43  Fri, 09 Dec 2016 04:00:15  154230465752        gif   \n",
       "44  Fri, 09 Dec 2016 02:00:30  154226290124        gif   \n",
       "45  Tue, 06 Dec 2016 14:00:22  154117927413        jpg   \n",
       "46  Tue, 06 Dec 2016 12:00:43  154115515601        jpg   \n",
       "47  Tue, 06 Dec 2016 10:00:40  154113363187        jpg   \n",
       "48  Tue, 06 Dec 2016 08:00:15  154110609304        jpg   \n",
       "49  Tue, 06 Dec 2016 06:00:14  154106783936        jpg   \n",
       "\n",
       "                                            photo-url  \\\n",
       "0   http://68.media.tumblr.com/b74fc499f2c4b2775db...   \n",
       "1   http://68.media.tumblr.com/59f9b61d12fd5bdada4...   \n",
       "2   http://68.media.tumblr.com/0d5dffb8caa823b1c8e...   \n",
       "3   http://68.media.tumblr.com/86292e8c9b2fffbfdc1...   \n",
       "4   http://68.media.tumblr.com/2761e107071e99101d3...   \n",
       "5   http://68.media.tumblr.com/64d8c99f8106d50b0df...   \n",
       "6   http://68.media.tumblr.com/db6cc862ec14d322866...   \n",
       "7   http://68.media.tumblr.com/0ba4cfd33cb1e45cd04...   \n",
       "8   http://68.media.tumblr.com/b411c3eab962762260d...   \n",
       "9   http://68.media.tumblr.com/dde5860cadf024ada4c...   \n",
       "10  http://68.media.tumblr.com/9065f1a1a9b843738ee...   \n",
       "11  http://68.media.tumblr.com/47b7f375b4149ab2bbd...   \n",
       "12  http://68.media.tumblr.com/4b46a4fe1028eb1ea93...   \n",
       "13  http://68.media.tumblr.com/9153d49c5566dada67f...   \n",
       "14  http://68.media.tumblr.com/89a1910a3816fad204b...   \n",
       "15  http://68.media.tumblr.com/cb202df1921795ccd3f...   \n",
       "16  http://68.media.tumblr.com/6b0d9fcc89c82c966e5...   \n",
       "17  http://68.media.tumblr.com/222188615a566104b71...   \n",
       "18  http://68.media.tumblr.com/6d571d7686f237400b8...   \n",
       "19  http://68.media.tumblr.com/d5cd8e8e6c242630ce7...   \n",
       "20  http://68.media.tumblr.com/a11791191e0812ce384...   \n",
       "21  http://68.media.tumblr.com/6b692cf29778f2e7f0d...   \n",
       "22  http://68.media.tumblr.com/ca686d3fd9620610d7f...   \n",
       "23  http://68.media.tumblr.com/af74e12415dc53f29b8...   \n",
       "24  http://68.media.tumblr.com/1367a126e535ea8ab9d...   \n",
       "25  http://68.media.tumblr.com/5176bff2db2a2347d9c...   \n",
       "26  http://68.media.tumblr.com/bab620eef666b11675a...   \n",
       "27  http://68.media.tumblr.com/b5d0166250e4a0d3488...   \n",
       "28  http://68.media.tumblr.com/de430bb20a063438fb9...   \n",
       "29  http://68.media.tumblr.com/d0396cd266b8cc19c37...   \n",
       "30  http://68.media.tumblr.com/ae42ce48ff8acf0dc93...   \n",
       "31  http://68.media.tumblr.com/5b1bb5a22fc533eb19a...   \n",
       "32  http://68.media.tumblr.com/483f023c438e5c8dd40...   \n",
       "33  http://68.media.tumblr.com/559b0e1ee203204a3a5...   \n",
       "34  http://68.media.tumblr.com/ee2920651e9b3d4e364...   \n",
       "35  http://68.media.tumblr.com/7567ddcbee4af447779...   \n",
       "36  http://68.media.tumblr.com/a2a9e83e75b3d8915ac...   \n",
       "37  http://68.media.tumblr.com/0c59f1654b492097c58...   \n",
       "38  http://68.media.tumblr.com/e1c705ca985b11c1e1a...   \n",
       "39  http://68.media.tumblr.com/bfc00e121ede86ec9d4...   \n",
       "40  http://68.media.tumblr.com/5f7e855d512ab2b2dc4...   \n",
       "41  http://68.media.tumblr.com/6fb9ab5f5b041be83d7...   \n",
       "42  http://68.media.tumblr.com/ba07f00e8ce8e4da765...   \n",
       "43  http://68.media.tumblr.com/7530c2210673a58e392...   \n",
       "44  http://68.media.tumblr.com/b2a5e62ef55a6d176c3...   \n",
       "45  http://68.media.tumblr.com/cc44f2ff65366ec6b81...   \n",
       "46  http://68.media.tumblr.com/fef80b257374162c757...   \n",
       "47  http://68.media.tumblr.com/e2fa3b7422a9873dff3...   \n",
       "48  http://68.media.tumblr.com/54068be23b407110229...   \n",
       "49  http://68.media.tumblr.com/3bb935758ee6eb31cdf...   \n",
       "\n",
       "                                  tags  \n",
       "0    [cat, cats, lol, lolcat, lolcats]  \n",
       "1    [cat, cats, lol, lolcat, lolcats]  \n",
       "2    [cat, cats, lol, lolcat, lolcats]  \n",
       "3    [cat, cats, lol, lolcat, lolcats]  \n",
       "4    [cat, cats, lol, lolcat, lolcats]  \n",
       "5    [cat, cats, lol, lolcat, lolcats]  \n",
       "6    [cat, cats, lol, lolcat, lolcats]  \n",
       "7    [cat, cats, lol, lolcat, lolcats]  \n",
       "8    [cat, cats, lol, lolcat, lolcats]  \n",
       "9    [cat, cats, lol, lolcat, lolcats]  \n",
       "10   [cat, cats, lol, lolcat, lolcats]  \n",
       "11   [cat, cats, lol, lolcat, lolcats]  \n",
       "12   [cat, cats, lol, lolcat, lolcats]  \n",
       "13   [cat, cats, lol, lolcat, lolcats]  \n",
       "14   [cat, cats, lol, lolcat, lolcats]  \n",
       "15   [cat, cats, lol, lolcat, lolcats]  \n",
       "16   [cat, cats, lol, lolcat, lolcats]  \n",
       "17   [cat, cats, lol, lolcat, lolcats]  \n",
       "18   [cat, cats, lol, lolcat, lolcats]  \n",
       "19   [cat, cats, lol, lolcat, lolcats]  \n",
       "20   [cat, cats, lol, lolcat, lolcats]  \n",
       "21   [cat, cats, lol, lolcat, lolcats]  \n",
       "22   [cat, cats, lol, lolcat, lolcats]  \n",
       "23   [cat, cats, lol, lolcat, lolcats]  \n",
       "24   [cat, cats, lol, lolcat, lolcats]  \n",
       "25   [cat, cats, lol, lolcat, lolcats]  \n",
       "26   [cat, cats, lol, lolcat, lolcats]  \n",
       "27   [cat, cats, lol, lolcat, lolcats]  \n",
       "28   [cat, cats, lol, lolcat, lolcats]  \n",
       "29   [cat, cats, lol, lolcat, lolcats]  \n",
       "30   [cat, cats, lol, lolcat, lolcats]  \n",
       "31   [cat, cats, lol, lolcat, lolcats]  \n",
       "32   [cat, cats, lol, lolcat, lolcats]  \n",
       "33   [cat, cats, lol, lolcat, lolcats]  \n",
       "34   [cat, cats, lol, lolcat, lolcats]  \n",
       "35   [cat, cats, lol, lolcat, lolcats]  \n",
       "36   [cat, cats, lol, lolcat, lolcats]  \n",
       "37   [cat, cats, lol, lolcat, lolcats]  \n",
       "38   [cat, cats, lol, lolcat, lolcats]  \n",
       "39   [cat, cats, lol, lolcat, lolcats]  \n",
       "40   [cat, cats, lol, lolcat, lolcats]  \n",
       "41   [cat, cats, lol, lolcat, lolcats]  \n",
       "42   [cat, cats, lol, lolcat, lolcats]  \n",
       "43  [gif, lolcat, lolcats, cat, funny]  \n",
       "44  [gif, lolcat, lolcats, cat, funny]  \n",
       "45   [cat, cats, lol, lolcat, lolcats]  \n",
       "46   [cat, cats, lol, lolcat, lolcats]  \n",
       "47   [cat, cats, lol, lolcat, lolcats]  \n",
       "48   [cat, cats, lol, lolcat, lolcats]  \n",
       "49   [cat, cats, lol, lolcat, lolcats]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting a max in case the blog has millions of images\n",
    "#The given max will be rounded up to the nearest multiple of 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    #Restating this here so the function isn't dependent on any external variables\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "    #There are a bunch of possible locations for the photo url\n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #These are the pieces of information we will be gathering,\n",
    "    #at the end we will convert this to a DataFrame.\n",
    "    #There are a few other datums we could gather like the captions\n",
    "    #you can read the Tumblr documentation to learn how to get them\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Tumblr limits us to a max of 50 posts per request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #We are dealing with uncleaned data, we can't trust it.\n",
    "            #Specifically, not all posts are guaranteed to have the fields we want\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Make sure your error messages are useful\n",
    "                #You will be one of the users\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Files\n",
    "\n",
    "What if the text we want isn't on a webpage? There are a many other sources of\n",
    "text available, typically organized into *files*.\n",
    "\n",
    "## Raw text (and encoding)\n",
    "\n",
    "The most basic form of storing text is as a _raw text_ document. Source code\n",
    "(`.py`, `.r`, etc) is usually raw text as are text files (`.txt`) and those with\n",
    "many other extension (e.g., .csv, .dat, etc.). Opening an unknown file with a\n",
    "text editor is often a great way of learning what the file is.\n",
    "\n",
    "We can create a text file in python with the `open()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, mode = 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice the `encoding='utf-8'` argument, which specifies how we map the bits from\n",
    "the file to the glyphs (and whitespace characters like tab (`'\\t'`) or newline\n",
    "(`'\\n'`)) on the screen. When dealing only with latin letters, arabic numerals\n",
    "and the other symbols on America keyboards you usually do not have to worry\n",
    "about encodings as the ones used today are backwards compatible with\n",
    "[ASCII](https://en.wikipedia.org/wiki/ASCII), which gives the binary\n",
    "representation of 128 characters.\n",
    "\n",
    "Some of you, however, will want to use other characters (e.g., Chinese\n",
    "characters). To solve this there is\n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode) which assigns numbers to\n",
    "symbols, e.g., 041 is `'A'` and 03A3 is `'Σ'` (numbers starting with 0 are\n",
    "hexadecimal). Often non/beyond-ASCII characters are called Unicode characters.\n",
    "Unicode contains 1,114,112 characters, about 10\\% of which have been assigned.\n",
    "Unfortunately there are many ways used to map combinations of bits to Unicode\n",
    "symbols. The ones you are likely to encounter are called by Python _utf-8_,\n",
    "_utf-16_ and _latin-1_. _utf-8_ is the standard for Linux and Mac OS while both\n",
    "_utf-16_ and _latin-1_ are used by windows. If you use the wrong encoding,\n",
    "characters can appear wrong, sometimes change in number or Python could raise an\n",
    "exception. Lets see what happens when we open the file we just created with\n",
    "different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with the correct encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n",
      "\n",
      "This is with the wrong encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols â¡ â â  â¡ â¢ â£ àµ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that with _latin-1_ the unicode characters are mixed up and there are too\n",
    "many of them. You need to keep in mind encoding when obtaining text files.\n",
    "Determining the encoding can sometime involve substantial work.\n",
    "\n",
    "## PDF\n",
    "\n",
    "Another common way text will be stored is in a PDF file. First we will download\n",
    "a pdf in Python. To do that lets grab a chapter from\n",
    "_Speech and Language Processing_, chapter 21 is on Information Extraction which\n",
    "seems apt. It is stored as a pdf at [https://web.stanford.edu/~jurafsky/slp3/21.\n",
    "pdf](https://web.stanford.edu/~jurafsky/slp3/21.pdf) although we are downloading from a copy just in case Jurafsky changes their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It says `'pdf'`, so thats a good sign. The rest though looks like we are having\n",
    "issues with an encoding. The random characters are not caused by our encoding\n",
    "being wrong, however. They are cause by there not being an encoding for those\n",
    "parts at all. PDFs are nominally binary files, meaning there are sections of\n",
    "binary that are specific to pdf and nothing else so you need something that\n",
    "knows about pdf to read them. To do that we will be using\n",
    "[`PyPDF2`](https://github.com/mstamy2/PyPDF2), a PDF processing library for\n",
    "Python 3.\n",
    "\n",
    "\n",
    "Because PDFs are a very complicated file format pdfminer requires a large amount\n",
    "of boilerplate code to extract text, we have written a function that takes in an\n",
    "open PDF file and returns the text so you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    #Using utf-8, if there are a bunch of random symbols try changing this\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we need to take the response object and convert it into a 'file like'\n",
    "object so that pdfminer can read it. To do this we will use `io`'s `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can give it to pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(readPDF(infoExtractionBytes)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From here we can either look at the full text or fiddle with our PDF reader and\n",
    "get more information about individual blocks of text.\n",
    "\n",
    "## Word Docs\n",
    "\n",
    "The other type of document you are likely to encounter is the `.docx`, these are\n",
    "actually a version of [XML](https://en.wikipedia.org/wiki/Office_Open_XML), just\n",
    "like HTML, and like HTML we will use a specialized parser.\n",
    "\n",
    "For this class we will use [`python-docx`](https://python-\n",
    "docx.readthedocs.io/en/latest/) which provides a nice simple interface for\n",
    "reading `.docx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#example_docx = 'https://github.com/KnowledgeLab/content_analysis/raw/data/example_doc.docx'\n",
    "\n",
    "r = requests.get(example_docx, stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This procedure uses the `io.BytesIO` class again, since `docx.Document` expects\n",
    "a file. Another way to do it is to save the document to a file and then read it\n",
    "like any other file. If we do this we can either delete the file afterwords, or\n",
    "save it and avoid downloading the following time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def downloadIfNeeded(targetURL, outputFile, **openkwargs):\n",
    "    if not os.path.isfile(outputFile):\n",
    "        outputDir = os.path.dirname(outputFile)\n",
    "        #This function is a more general os.mkdir()\n",
    "        os.makedirs(outputDir, exist_ok = True)\n",
    "        r = requests.get(targetURL, stream=True)\n",
    "        #Using a closure like this is generally better than having to\n",
    "        #remember to close the file. There are ways to make this function\n",
    "        #work as a closure too\n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return open(outputFile, **openkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function will download, save and open `outputFile` as `outputFile` or just\n",
    "open it if `outputFile` exists. By default `open()` will open the file as read\n",
    "only text with the local encoding, which may cause issues if its not a text\n",
    "file. This next cell will raise an error if run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#d = docx.Document(downloadIfNeeded(example_docx, example_docx_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to tell `open()` to read in binary mode (`'rb'`), this is why we added\n",
    "`**openkwargs`, this allows us to pass any keyword arguments (kwargs) from\n",
    "`downloadIfNeeded` to `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb'))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the file with `docx.Document` and not have to wait for it to be\n",
    "downloaded every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "We will now look at some ensemble classification methods. To start with lets us another API to get our data, the reddit API. There is a Python package that makes using the API simple called `praw`, but it requires an account. The data required are in `data/redditOAUTH.json` and we can load it from the file. Our targeted subreddits are: 'talesfromtechsupport', 'badroommates', 'weeabootales' and 'relationships'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = ['talesfromtechsupport', 'badroommates', 'weeabootales', 'relationships']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data we will define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopN(n, name, reddit):\n",
    "    sub = reddit.subreddit(name)\n",
    "    subIter = sub.top('all', limit=None)\n",
    "\n",
    "    dfDict = {\n",
    "        'text' : [],\n",
    "        'score' : [],\n",
    "        'author' : [],\n",
    "        'title' : [],\n",
    "        'url' : [],\n",
    "        'over_18' : [],\n",
    "        'subreddit' : [],\n",
    "    }\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            post = next(subIter)\n",
    "            while post.stickied or post.media is not None or 'reddit.com/r/{}'.format(name) not in post.url:\n",
    "                post = next(subIter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        print(\"Getting {}: {}\".format(name, i))\n",
    "        dfDict['text'].append(post.selftext.replace('\\n', ' '))\n",
    "        dfDict['score'].append(post.score)\n",
    "        try:\n",
    "            dfDict['author'].append(post.author.name)\n",
    "        except AttributeError:\n",
    "            dfDict['author'].append('[deleted]')\n",
    "        dfDict['title'].append(post.title)\n",
    "        dfDict['url'].append(post.url)\n",
    "        dfDict['over_18'].append(post.over_18)\n",
    "        dfDict['subreddit'].append(post.subreddit.title)\n",
    "    return pandas.DataFrame(dfDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can call it, but each story will take about a second, due to reddit's rate limiting, so we have the data already downloaded for you. If you want want to download it just change `download = False` to `Download = True` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "download = False\n",
    "if download:\n",
    "    with open('data/redditOAUTH.json') as f:\n",
    "        reddit = praw.Reddit(**json.loads(f.read()))\n",
    "\n",
    "    df = pandas.DataFrame()\n",
    "    for target in targets:\n",
    "        df = df.append(getTopN(400, target, reddit), ignore_index=True)\n",
    "        df.to_csv('redditDat.csv')\n",
    "        \n",
    "#Load and drop a couple missing values\n",
    "redditDf = pandas.read_csv('data/reddit.csv', index_col = 0).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is text data we need to convert it into a vector, to start with we will convert the strings into lists of strings and clean it, called tokenizing and stemming, respectively. To do this we can use the following function based on nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = nltk.corpus.stopwords.words('english'), stemmer = nltk.stem.snowball.SnowballStemmer('english'), lemmer = None, vocab = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "        \n",
    "    #We will return a list with the stopwords removed\n",
    "    if vocab is not None:\n",
    "        vocab_str = '|'.join(vocab)\n",
    "        workingIter = (w for w in workingIter if re.match(vocab_str, w))\n",
    "    \n",
    "    return list(workingIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "redditDf['tokenized_text'] = redditDf['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "#Normalize\n",
    "redditDf['normalized_text'] = redditDf['tokenized_text'].apply(lambda x: normlizeTokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have nice lists of string we stinng need to convert them into vectors, we could just count each word's number of occurneces and use that, but that leads to very sparse vectors. So we will use [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)(TF-IDF) which uses the frequencies of occurnece in the lists diveided by the total number of occurneces. We will also remove words with too high or too low frequencies so that the vectors are dense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')\n",
    "redditTFVects = redditTFVectorizer.fit_transform([' '.join(l) for l in redditDf['normalized_text']])\n",
    "redditDf['tfVect'] = [np.array(v) for v in redditTFVects.todense()]\n",
    "redditDf['tfVect'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a collection of vectors to work on. We now need to create a testing set and a trainign set, this function will let us do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getholdOut(df, split = .7):\n",
    "    sf = df.reindex(np.random.permutation(df.index))\n",
    "    train = sf[:int(len(df) * split)].copy()\n",
    "    train.index = range(len(train))\n",
    "    test = sf[int(len(df) * split):].copy()\n",
    "    test.index = range(len(test))\n",
    "    return train, test\n",
    "redditDfTraining, redditDfTesting = getholdOut(redditDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few models, lets start with a decision tree\n",
    "\n",
    "All the models in sklearn follow the same usage pattern, first you specify your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decisionTree = sklearn.tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can provide the model with the traning data, in this case we need two things, the vectors and the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decisionTree.fit(np.stack(redditDfTraining['tfVect'], axis=1)[0], redditDfTraining['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a fully trained decision tree, lets test it on one of our testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetRow = 10\n",
    "print(redditDfTesting.iloc[targetRow])\n",
    "print(\"Is predicted to be:\")\n",
    "print(decisionTree.predict(redditDfTesting['tfVect'][targetRow]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only a single value is OK, bu what we really want to do is look at all of the testing set, tow do this we first need to get all the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDfTesting['predictionsDT'] = decisionTree.predict(np.stack(redditDfTesting['tfVect'], axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can look at a couple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'precision')\n",
    "print(sklearn.metrics.recall_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'recall')\n",
    "print(sklearn.metrics.f1_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'F-1 measure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sklearn.metrics.confusion_matrix(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=targets, yticklabels=targets)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we do better?\n",
    "\n",
    "Lets look at random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomForest = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomForest.fit(np.stack(redditDfTraining['tfVect'], axis=1)[0], redditDfTraining['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditDfTesting['predictionsRF'] = randomForest.predict(np.stack(redditDfTesting['tfVect'], axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And score them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(redditDfTesting['subreddit'], redditDfTesting['predictionsRF'], average = 'weighted'), 'precision')\n",
    "print(sklearn.metrics.recall_score(redditDfTesting['subreddit'], redditDfTesting['predictionsRF'], average = 'weighted'), 'recall')\n",
    "print(sklearn.metrics.f1_score(redditDfTesting['subreddit'], redditDfTesting['predictionsRF'], average = 'weighted'), 'F-1 measure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sklearn.metrics.confusion_matrix(redditDfTesting['subreddit'], redditDfTesting['predictionsRF'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=targets, yticklabels=targets)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the random forest is a good bit better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boosting = randomForest = sklearn.ensemble.GradientBoostingClassifier()\n",
    "boosting.fit(np.stack(redditDfTraining['tfVect'], axis=1)[0], redditDfTraining['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDfTesting['predictionsBoost'] = boosting.predict(np.stack(redditDfTesting['tfVect'], axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(redditDfTesting['subreddit'], redditDfTesting['predictionsBoost'], average = 'weighted'), 'precision')\n",
    "print(sklearn.metrics.recall_score(redditDfTesting['subreddit'], redditDfTesting['predictionsBoost'], average = 'weighted'), 'recall')\n",
    "print(sklearn.metrics.f1_score(redditDfTesting['subreddit'], redditDfTesting['predictionsBoost'], average = 'weighted'), 'F-1 measure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sklearn.metrics.confusion_matrix(redditDfTesting['subreddit'], redditDfTesting['predictionsBoost'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=targets, yticklabels=targets)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a bit longer to train, but the results are better, what about a combining differnet classifiers with stacking?\n",
    "\n",
    "To do this we first selected the classifiers then look at their votes for the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('decisionTree', sklearn.tree.DecisionTreeClassifier()),\n",
    "    ('randomForest', sklearn.ensemble.RandomForestClassifier()),\n",
    "    #('boosting', sklearn.ensemble.GradientBoostingClassifier()),\n",
    "    ('lin', sklearn.linear_model.LinearRegression())\n",
    "]\n",
    "\n",
    "stacking = sklearn.ensemble.VotingClassifier(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can treat this classifiers the same as all the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacking.fit(np.stack(redditDfTraining['tfVect'], axis=1)[0], redditDfTraining['subreddit'])\n",
    "redditDfTesting['predictionsStacking'] = boosting.predict(np.stack(redditDfTesting['tfVect'], axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(redditDfTesting['subreddit'], redditDfTesting['predictionsStacking'], average = 'weighted'), 'precision')\n",
    "print(sklearn.metrics.recall_score(redditDfTesting['subreddit'], redditDfTesting['predictionsStacking'], average = 'weighted'), 'recall')\n",
    "print(sklearn.metrics.f1_score(redditDfTesting['subreddit'], redditDfTesting['predictionsStacking'], average = 'weighted'), 'F-1 measure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = sklearn.metrics.confusion_matrix(redditDfTesting['subreddit'], redditDfTesting['predictionsStacking'])\n",
    "seaborn.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=targets, yticklabels=targets)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
